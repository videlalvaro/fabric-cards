(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[931],{4576:function(e,a,t){Promise.resolve().then(t.bind(t,8697))},8697:function(e,a,t){"use strict";t.r(a),t.d(a,{default:function(){return Home}});var o=t(7437),r=t(2265),i=t(7042),s=t(4769);function cn(){for(var e=arguments.length,a=Array(e),t=0;t<e;t++)a[t]=arguments[t];return(0,s.m6)((0,i.W)(a))}let n=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,o.jsx)("div",{ref:a,className:cn("rounded-xl border bg-card text-card-foreground shadow",t),...r})});n.displayName="Card";let c=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,o.jsx)("div",{ref:a,className:cn("flex flex-col space-y-1.5 p-6",t),...r})});c.displayName="CardHeader";let d=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,o.jsx)("h3",{ref:a,className:cn("font-semibold leading-none tracking-tight",t),...r})});d.displayName="CardTitle";let l=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,o.jsx)("p",{ref:a,className:cn("text-sm text-muted-foreground",t),...r})});l.displayName="CardDescription";let u=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,o.jsx)("div",{ref:a,className:cn("p-6 pt-0",t),...r})});u.displayName="CardContent";let h=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,o.jsx)("div",{ref:a,className:cn("flex items-center p-6 pt-0",t),...r})});h.displayName="CardFooter";var m=t(6248),p=t.n(m),f=t(4972),b=t(6061);let g=(0,b.j)("inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50",{variants:{variant:{default:"bg-primary text-primary-foreground shadow hover:bg-primary/90",destructive:"bg-destructive text-destructive-foreground shadow-sm hover:bg-destructive/90",outline:"border border-input bg-transparent shadow-sm hover:bg-accent hover:text-accent-foreground",secondary:"bg-secondary text-secondary-foreground shadow-sm hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-9 px-4 py-2",sm:"h-8 rounded-md px-3 text-xs",lg:"h-10 rounded-md px-8",icon:"h-9 w-9"}},defaultVariants:{variant:"default",size:"default"}}),y=r.forwardRef((e,a)=>{let{className:t,variant:r,size:i,asChild:s=!1,...n}=e,c=s?f.g7:"button";return(0,o.jsx)(c,{className:cn(g({variant:r,size:i,className:t})),ref:a,...n})});y.displayName="Button";var w=t(1396),A=t.n(w);function FlashCard(e){let{QandAs:a,topic:t,moduleTitle:i}=e,s=a[t],[l,m]=(0,r.useState)(!1),[f,b]=(0,r.useState)(0);function flipCard(){m(!l)}function nextCard(){let e=(f+1)%s.length;b(e),m(!1)}function previousCard(){let e=(f-1+s.length)%s.length;b(e),m(!1)}function createLearnLink(e){return"https://learn.microsoft.com/training/modules/"+t+"/"+e+"?WT.mc_id=data-111905-alvidela"}return(0,o.jsx)("div",{id:"flashcard",className:"flex justify-center items-center h-screen bg-gray-100 dark:bg-gray-900",children:(0,o.jsxs)(n,{className:"w-96 bg-[#9ee0cb] rounded-xl shadow-md overflow-hidden m-4",children:[(0,o.jsx)("h1",{className:"text-lg m-5",children:i}),(0,o.jsxs)(p(),{isFlipped:l,children:[(0,o.jsx)("div",{className:"react-card-front bg-[#c0ecdd]",children:(0,o.jsxs)("div",{className:"p-8",children:[(0,o.jsx)(c,{children:(0,o.jsx)(d,{className:"text-[#001919]",children:"Question"})}),(0,o.jsx)(u,{className:"text-sm text-[#001919] h-44 overflow-auto",children:s[f].Q}),(0,o.jsx)(y,{className:"mt-4 text-[#001919] border-[#001919]",variant:"outline",onClick:flipCard,children:"Show Answer"})]})}),(0,o.jsx)("div",{className:"react-card-back bg-[#c0ecdd]",children:(0,o.jsxs)("div",{className:"p-8",children:[(0,o.jsx)(c,{children:(0,o.jsx)(d,{className:"text-[#001919]",children:"Answer"})}),(0,o.jsxs)(u,{className:"text-sm text-[#001919] h-44 overflow-auto",children:[s[f].A,(0,o.jsx)("br",{}),(0,o.jsx)("br",{}),(0,o.jsx)(A(),{className:"bg-green-100 text-green-800 text-xs font-medium me-2 px-2.5 py-0.5 rounded dark:bg-green-900 dark:text-green-300",href:createLearnLink(s[f].source),rel:"noopener noreferrer",target:"_blank",children:"Learn More"})]}),(0,o.jsx)(y,{className:"mt-4 text-[#001919] border-[#001919]",variant:"outline",onClick:flipCard,children:"Hide Answer"})]})})]}),(0,o.jsxs)(h,{className:"flex justify-between p-4 bg-[#9ee0cb]",children:[(0,o.jsx)(y,{className:"bg-[#117865] text-white border-[#8B4513]",onClick:previousCard,children:"Previous"}),(0,o.jsx)(y,{className:"bg-[#117865] text-white",onClick:nextCard,children:"Next"})]})]})})}function Topic(e){let{topics:a,onTopicChange:t,onModuleTitleChange:i}=e,s=["bg-[#9ee0cb]","bg-[#c0ecdd]"],l=(0,r.useCallback)(e=>{e.preventDefault(),t(e.currentTarget.getAttribute("data-module")),i(e.currentTarget.getAttribute("data-title")),scrollToFlashcard()},[t,i]);function scrollToFlashcard(){let e=document.getElementById("flashcard");e.scrollIntoView({behavior:"smooth"})}function createLearnLink(e){return"https://learn.microsoft.com/training/modules/"+e+"/?WT.mc_id=data-111905-alvidela"}return(0,o.jsx)("div",{className:"p-4",children:(0,o.jsx)("div",{className:"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4",children:a.map((e,a)=>(0,o.jsxs)(n,{className:s[a%s.length],children:[(0,o.jsx)(c,{children:(0,o.jsx)(d,{children:e.title})}),(0,o.jsx)(u,{className:"text-sm",children:e.summary}),(0,o.jsxs)(h,{children:[(0,o.jsx)(A(),{className:"bg-green-100 text-green-800 text-xs font-medium me-2 px-2.5 py-0.5 rounded dark:bg-green-900 dark:text-green-300",href:createLearnLink(e.module),rel:"noopener noreferrer",target:"_blank",children:"Go through the Learn Module"}),(0,o.jsx)("div",{style:{marginRight:"1em"}}),(0,o.jsx)(A(),{className:"bg-green-100 text-green-800 text-xs font-medium me-2 px-2.5 py-0.5 rounded dark:bg-green-900 dark:text-green-300",href:"#",onClick:l,"data-module":e.module,"data-title":e.title,children:"Practice with Flashcards"})]})]},a))})})}var k=JSON.parse('{"introduction-end-analytics-use-microsoft-fabric":[{"Q":"What is Microsoft Fabric?","A":"Microsoft Fabric is an end-to-end analytics platform.","source":"1-introduction"},{"Q":"What does Microsoft Fabric provide?","A":"Microsoft Fabric provides a single, integrated environment for data professionals and the business to collaborate on data projects.","source":"1-introduction"},{"Q":"What are the services included in Microsoft Fabric?","A":"The services include data engineering, data integration, data warehousing, real-time analytics, data science, and business intelligence.","source":"1-introduction"},{"Q":"Who is Microsoft Fabric for?","A":"Microsoft Fabric is for both citizen and professional data practitioners.","source":"1-introduction"},{"Q":"What does Microsoft Fabric integrate with?","A":"Microsoft Fabric integrates with tools the business needs to make decisions.","source":"1-introduction"},{"Q":"What is the benefit of using Microsoft Fabric?","A":"Scalability, cost-effectiveness, accessibility, and continuous updates and maintenance.","source":"2-explore-analytics-fabric"},{"Q":"What is OneLake?","A":"OneLake is Fabric\'s lake-centric architecture that provides a single, integrated environment for data professionals and the business to collaborate on data projects.","source":"2-explore-analytics-fabric"},{"Q":"What formats can be stored in OneLake?","A":"Data can be stored in any format, including Delta, Parquet, CSV, JSON, and more.","source":"2-explore-analytics-fabric"},{"Q":"What are some of the analytics experiences offered by Fabric?","A":"Synapse Data Engineering, Synapse Data Warehouse, Synapse Data Science, Synapse Real-Time Analytics, Data Factory, and Power BI.","source":"2-explore-analytics-fabric"},{"Q":"Where can Fabric administration be managed?","A":"Fabric administration can be managed in the admin center.","source":"2-explore-analytics-fabric"},{"Q":"What does Microsoft Fabric\'s unified management and governance make easier?","A":"It makes it easier for data professionals to work together on data projects.","source":"3-data-team"},{"Q":"What does Fabric remove?","A":"Fabric removes data silos and the need for access to multiple systems.","source":"3-data-team"},{"Q":"What does Fabric enhance?","A":"Fabric enhances collaboration between data professionals.","source":"3-data-team"},{"Q":"What role separation does Fabric eliminate?","A":"Fabric eliminates the separation between data engineers and data analysts.","source":"3-data-team"},{"Q":"How does Fabric help data analysts with downstream data transformations?","A":"Fabric allows data analysts to see the lineage and connect with data more directly with DirectLake mode.","source":"3-data-team"},{"Q":"What are the permissions required to enable Microsoft Fabric?","A":"Fabric admin, Power Platform admin, Microsoft 365 admin","source":"4-use-fabric"},{"Q":"Can Fabric be enabled at the capacity level?","A":"Yes, it can be enabled for specific groups of users","source":"4-use-fabric"},{"Q":"What should you do if you don\'t have access to Fabric?","A":"Contact the Fabric administrator to find out if it\'s available to you","source":"4-use-fabric"},{"Q":"Where can you access the Admin center to enable Microsoft Fabric?","A":"From the Settings menu in the upper right corner of the Power BI service","source":"4-use-fabric"},{"Q":"In which capacity must workspaces be to use Fabric?","A":"Premium capacity","source":"4-use-fabric"}],"get-started-lakehouses":[{"Q":"What is the foundation of Microsoft Fabric?","A":"Lakehouse","source":"1-introduction"},{"Q":"What is a lakehouse?","A":"A unified platform that combines the storage of a data lake and the ability to query and analyze data of a data warehouse","source":"1-introduction"},{"Q":"What are some sources of unstructured data?","A":"Social media, website logs, and third-party sources","source":"1-introduction"},{"Q":"Why did the company choose Microsoft Fabric?","A":"To improve decision-making capabilities by analyzing data in various formats across multiple sources","source":"1-introduction"},{"Q":"What can you query in a Microsoft Fabric lakehouse?","A":"Files and tables using SQL","source":"1-introduction"},{"Q":"What is a lakehouse?","A":"A lakehouse is a database built on top of a data lake using Delta format tables.","source":"2-fabric-lakehouse"},{"Q":"What are some benefits of a lakehouse?","A":"Some benefits of a lakehouse include using Spark and SQL engines for processing large-scale data, support for ACID transactions, and being a single location for data engineers, scientists, and analysts.","source":"2-fabric-lakehouse"},{"Q":"What is a schema-on-read format?","A":"A schema-on-read format means that the schema is defined as needed, rather than having a predefined schema.","source":"2-fabric-lakehouse"},{"Q":"What can you do with a lakehouse?","A":"You can query the data using SQL, use it for machine learning models, perform real-time analytics, and develop reports in Power BI.","source":"2-fabric-lakehouse"},{"Q":"What tools can you use to explore and transform data in a lakehouse?","A":"You can use Notebooks or Dataflows (Gen2) to explore and transform data in a lakehouse.","source":"2-fabric-lakehouse"},{"Q":"What are the three named items produced in a Fabric-enabled workspace?","A":"Lakehouse, Dataset (default), SQL Endpoint","source":"3-work-lakehouse"},{"Q":"What are the two modes in which you can work with data in a lakehouse?","A":"Lakehouse, SQL Endpoint","source":"3-work-lakehouse"},{"Q":"What are the ways to load data into a Fabric lakehouse?","A":"Upload, Dataflows (Gen2), Notebooks, Data Factory pipelines","source":"3-work-lakehouse"},{"Q":"What are shortcuts in Fabric used for?","A":"To integrate data into the lakehouse while keeping it stored in external storage","source":"3-work-lakehouse"},{"Q":"Where can shortcuts be created?","A":"In both Lakehouses and KQL databases","source":"3-work-lakehouse"},{"Q":"What is an Apache Spark?","A":"Apache Spark is a tool used to process data in lakehouse using Scala, PySpark, or Spark SQL.","source":"4-explore-data-lakehouse"},{"Q":"What are Notebooks?","A":"Notebooks are interactive coding interfaces where you can read, transform, and write data directly to the lakehouse.","source":"4-explore-data-lakehouse"},{"Q":"How can you explore data in lakehouse tables?","A":"You can use the SQL endpoint to run Transact-SQL statements to query and explore data.","source":"4-explore-data-lakehouse"},{"Q":"What are Dataflows (Gen2)?","A":"Dataflows are used to perform transformations through Power Query and land the transformed data back to the lakehouse.","source":"4-explore-data-lakehouse"},{"Q":"What are data pipelines?","A":"Data pipelines are used to orchestrate complex data transformation logic on data in the lakehouse using activities like dataflows and Spark jobs.","source":"4-explore-data-lakehouse"}],"use-apache-spark-work-files-lakehouse":[{"Q":"What is Apache Spark?","A":"Apache Spark is an open source parallel processing framework for large-scale data processing and analytics.","source":"1-introduction"},{"Q":"Where can Spark be used?","A":"Spark is available in multiple platform implementations, including Azure HDInsight, Azure Databricks, Azure Synapse Analytics, and Microsoft Fabric.","source":"1-introduction"},{"Q":"What does this module explore?","A":"This module explores how you can use Spark in Microsoft Fabric to ingest, process, and analyze data in a lakehouse.","source":"1-introduction"},{"Q":"What is the advantage of using Spark in the same environment as other data services in Microsoft Fabric?","A":"It makes it easier to incorporate Spark-based data processing into your overall data analytics solution.","source":"1-introduction"},{"Q":"Are the core techniques and code described in this module specific to Microsoft Fabric?","A":"No, the core techniques and code described in this module are common to all Spark implementations.","source":"1-introduction"},{"Q":"What is Apache Spark?","A":"Apache Spark is a distributed data processing framework.","source":"2-spark"},{"Q":"How does Spark process large volumes of data quickly?","A":"Spark uses a \'divide and conquer\' approach by distributing the work across multiple computers.","source":"2-spark"},{"Q":"What is the driver program in Spark?","A":"The driver program is the code that initiates a data processing job in Spark.","source":"2-spark"},{"Q":"What languages can be used with Spark?","A":"Java, Scala, Spark R, Spark SQL, and PySpark.","source":"2-spark"},{"Q":"What can you do to manage Spark settings in Microsoft Fabric?","A":"You can manage Spark settings in the Data Engineering/Science section of the workspace settings.","source":"2-spark"},{"Q":"What are the two ways to edit and run Spark code in Microsoft Fabric?","A":"Notebooks and Spark job","source":"3-spark-code"},{"Q":"What can you combine in a notebook?","A":"Text, images, and code written in multiple languages","source":"3-spark-code"},{"Q":"What are the two types of content that cells in a notebook can contain?","A":"Markdown-formatted content or executable code","source":"3-spark-code"},{"Q":"When using a notebook, can you see the results of the code immediately?","A":"Yes","source":"3-spark-code"},{"Q":"When would you use a Spark job definition?","A":"To ingest and transform data as part of an automated process","source":"3-spark-code"},{"Q":"What is the most commonly used data structure for working with structured data in Spark?","A":"Dataframe","source":"4-dataframe"},{"Q":"What is the equivalent Scala code for loading the products data example?","A":"val df = spark.read.format(\\"csv\\").option(\\"header\\", \\"true\\").load(\\"Files/data/products.csv\\")","source":"4-dataframe"},{"Q":"How can you specify an explicit schema when loading data into a dataframe?","A":"By using the \'schema\' parameter in the \'load\' method","source":"4-dataframe"},{"Q":"What method can be used to filter columns in a dataframe?","A":"select","source":"4-dataframe"},{"Q":"How can you save a dataframe as a partitioned set of files?","A":"By using the \'partitionBy\' method when writing the data","source":"4-dataframe"},{"Q":"What is the purpose of the Spark catalog?","A":"The Spark catalog is a metastore for relational data objects such as views and tables.","source":"5-spark-sql"},{"Q":"How can you make data in a dataframe available for querying in the Spark catalog?","A":"One way is to create a temporary view with the \'createOrReplaceTempView\' method.","source":"5-spark-sql"},{"Q":"What is the difference between a temporary view and a table in the Spark catalog?","A":"A temporary view is automatically deleted at the end of the session, while a table is persisted in the catalog and can be queried using Spark SQL.","source":"5-spark-sql"},{"Q":"What is the preferred format for tables in Microsoft Fabric?","A":"The preferred format is \'delta\', which is the format for a relational data technology on Spark named Delta Lake.","source":"5-spark-sql"},{"Q":"How can you query data from the catalog using the Spark SQL API?","A":"You can use the \'spark.sql\' method and a SQL query to return data as a dataframe.","source":"5-spark-sql"},{"Q":"What are some basic charting capabilities in Microsoft Fabric?","A":"Notebooks in Microsoft Fabric have basic charting capabilities.","source":"6-visualize-data"},{"Q":"When the built-in charting functionality doesn\'t provide what you need, what can you use to create data visualizations?","A":"You can use one of the many Python graphics libraries.","source":"6-visualize-data"},{"Q":"What is a popular graphics library in Python for creating data visualizations?","A":"Matplotlib is a popular graphics library in Python.","source":"6-visualize-data"},{"Q":"What method is used to convert a Spark dataframe to a Pandas dataframe in the example code?","A":"The `toPandas` method is used to convert the dataframe.","source":"6-visualize-data"},{"Q":"What is an example of a highly customized charting library?","A":"Seaborn is an example of a highly customized charting library.","source":"6-visualize-data"}],"work-delta-lake-tables-fabric":[{"Q":"What is Delta Lake?","A":"Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data.","source":"1-introduction"},{"Q":"What is the advantage of using Delta Lake in a lakehouse architecture?","A":"Delta Lake offers the advantages of a relational database system with the flexibility of data file storage in a data lake.","source":"1-introduction"},{"Q":"Do you need to work directly with Delta Lake APIs to use tables in a Fabric lakehouse?","A":"No, you don\'t need to work directly with Delta Lake APIs to use tables in a Fabric lakehouse.","source":"1-introduction"},{"Q":"What is the purpose of the Delta Lake metastore architecture?","A":"The Delta Lake metastore architecture enables advanced analytics solutions on Microsoft Fabric.","source":"1-introduction"},{"Q":"What are some specialized Delta table operations?","A":"Specialized Delta table operations can greatly expand your ability to build advanced analytics solutions on Microsoft Fabric.","source":"1-introduction"},{"Q":"What is Delta Lake?","A":"Delta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing.","source":"2-understand-delta-lake"},{"Q":"What does the Delta icon signify for tables in a Microsoft Fabric lakehouse?","A":"The Delta icon signifies that the tables are Delta tables.","source":"2-understand-delta-lake"},{"Q":"What is stored in the _delta_Log folder for each table?","A":"Transaction details are logged in JSON format in the _delta_Log folder.","source":"2-understand-delta-lake"},{"Q":"What are the benefits of using Delta tables?","A":"The benefits include support for querying and data modification, ACID transactions, data versioning and time travel, support for batch and streaming data, and standard formats and interoperability.","source":"2-understand-delta-lake"},{"Q":"What format is the underlying data for Delta tables stored in?","A":"The underlying data is stored in Parquet format.","source":"2-understand-delta-lake"},{"Q":"How can you create a delta table in Spark?","A":"You can save a dataframe in delta format.","source":"3-create-delta-tables"},{"Q":"What is the difference between managed and external tables?","A":"Managed tables have the table definition and data files both managed by Spark, while external tables have the table definition mapped to an alternative file storage location.","source":"3-create-delta-tables"},{"Q":"How can you create a table definition in the metastore?","A":"You can use the DeltaTableBuilder API or the Spark SQL CREATE TABLE statement.","source":"3-create-delta-tables"},{"Q":"What is the advantage of saving data in delta format without creating a table definition?","A":"It allows you to persist the results of data transformations and later overlay a table definition or process directly using the delta lake API.","source":"3-create-delta-tables"},{"Q":"What happens when you save a dataframe to the Tables location in the lakehouse?","A":"The corresponding table metadata is automatically created in the metastore.","source":"3-create-delta-tables"},{"Q":"What is the most common way to work with data in delta tables in Spark?","A":"Using Spark SQL.","source":"4-work-delta-data"},{"Q":"How can you insert a row into the products table using Spark SQL?","A":"By using the `spark.sql` library and executing an SQL statement.","source":"4-work-delta-data"},{"Q":"What API can you use to work with delta format files?","A":"The Delta Lake API.","source":"4-work-delta-data"},{"Q":"How can you update the data in a delta table using the Delta Lake API?","A":"By creating a DeltaTable object and using the `update` method.","source":"4-work-delta-data"},{"Q":"How can you retrieve older versions of the data in a delta table?","A":"By using *time travel* and specifying the version or timestamp required.","source":"4-work-delta-data"},{"Q":"What is Spark Structured Streaming?","A":"Spark Structured Streaming is an API in Spark that allows for streaming data processing.","source":"5-use-delta-lake-streaming-data"},{"Q":"What are the types of streaming sources that Spark Structured Streaming can read from?","A":"Spark Structured Streaming can read data from network ports, real time message brokering services, or file system locations.","source":"5-use-delta-lake-streaming-data"},{"Q":"What is the purpose of using a delta table as a streaming source?","A":"Using a delta table as a streaming source allows you to constantly report new data as it is added to the table.","source":"5-use-delta-lake-streaming-data"},{"Q":"What operations can be included in a delta table used as a streaming source?","A":"When using a delta table as a streaming source, only append operations can be included in the stream.","source":"5-use-delta-lake-streaming-data"},{"Q":"What is the purpose of the checkpointLocation option when writing a stream to a delta table?","A":"The checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing.","source":"5-use-delta-lake-streaming-data"}],"use-data-factory-pipelines-fabric":[{"Q":"What is the purpose of data pipelines?","A":"To extract, transform, and load data from one or more sources to a destination","source":"1-introduction"},{"Q":"What is the role of pipelines in ETL processes?","A":"To automate the ingestion of transactional data into an analytical data store","source":"1-introduction"},{"Q":"What is the relationship between Azure Data Factory and Microsoft Fabric?","A":"Data pipelines in Microsoft Fabric use the same architecture as Azure Data Factory","source":"1-introduction"},{"Q":"How can pipelines be executed in Microsoft Fabric?","A":"Interactively in the user interface or scheduled to run automatically","source":"1-introduction"},{"Q":"What kind of tasks can be included in a data pipeline?","A":"Multiple kinds of data processing tasks and control flow logic","source":"1-introduction"},{"Q":"What are pipelines in Microsoft Fabric?","A":"Pipelines in Microsoft Fabric encapsulate a sequence of activities that perform data movement and processing tasks.","source":"2-understand-fabric-pipeline"},{"Q":"What is the purpose of a pipeline?","A":"A pipeline is used to define data transfer and transformation activities, and orchestrate these activities through control flow activities.","source":"2-understand-fabric-pipeline"},{"Q":"What are the two broad categories of activities in a pipeline?","A":"The two broad categories of activities in a pipeline are data transformation activities and control flow activities.","source":"2-understand-fabric-pipeline"},{"Q":"What is the purpose of parameters in a pipeline?","A":"Parameters enable you to provide specific values to be used each time a pipeline is run, increasing the reusability of your pipelines.","source":"2-understand-fabric-pipeline"},{"Q":"What is a pipeline run?","A":"A pipeline run is initiated each time a pipeline is executed, and can be reviewed to confirm completion and investigate settings used.","source":"2-understand-fabric-pipeline"},{"Q":"What is the most common use of a data pipeline?","A":"Copy data activity","source":"3-copy-data"},{"Q":"How can you create a repeatable data ingestion process?","A":"Combine copy data activity with other activities","source":"3-copy-data"},{"Q":"What is the purpose of the Copy Data tool?","A":"To configure the data source and destination for the copy operation","source":"3-copy-data"},{"Q":"When should you use the Copy Data activity?","A":"When you need to copy data directly without transformations or when you want to import raw data","source":"3-copy-data"},{"Q":"What activity can you use to apply transformations to ingested data?","A":"Data Flow activity","source":"3-copy-data"},{"Q":"Can you create custom data ingestion and transformation processes?","A":"Yes, you can define pipelines from any combination of activities you choose.","source":"4-pipeline-templates"},{"Q":"Are there predefined pipeline templates?","A":"Yes, Microsoft Fabric includes predefined pipeline templates.","source":"4-pipeline-templates"},{"Q":"How can you create a pipeline based on a template?","A":"Select the \'Choose a task to start\' tile in a new pipeline.","source":"4-pipeline-templates"},{"Q":"What options are displayed when you select \'Choose a task to start\'?","A":"A selection of pipeline templates is displayed.","source":"4-pipeline-templates"},{"Q":"Can you customize a pipeline template?","A":"Yes, you can edit the pipeline in the pipeline canvas to customize it.","source":"4-pipeline-templates"},{"Q":"What can you do after completing a pipeline?","A":"You can validate it and run it interactively or schedule it.","source":"5-run-monitor-pipelines"},{"Q":"Where can you view the run history for a pipeline?","A":"You can view it from the pipeline canvas or from the pipeline item listed in the workspace page.","source":"5-run-monitor-pipelines"},{"Q":"What details can you see when viewing a pipeline run history from the workspace page?","A":"You can see the Run start value and view the individual execution time for each activity as a Gantt chart.","source":"5-run-monitor-pipelines"},{"Q":"What can you see in a screenshot of the Run menu for a pipeline?","A":"You can see options like Validate, Run interactively, and Specify schedule.","source":"5-run-monitor-pipelines"},{"Q":"What can you see in a screenshot of a pipeline run history?","A":"You can see details of each run, such as start time and duration.","source":"5-run-monitor-pipelines"}],"use-dataflow-gen-2-fabric":[{"Q":"What is Microsoft Fabric?","A":"Microsoft Fabric offers a unified solution for data engineering, integration, and analytics.","source":"1-introduction"},{"Q":"What is the purpose of Dataflows (Gen2)?","A":"Dataflows (Gen2) are used to ingest and transform data from multiple sources, and then land the cleansed data to another destination.","source":"1-introduction"},{"Q":"Why are dataflows important in end-to-end analytics?","A":"Dataflows are important in end-to-end analytics because they allow data engineers to prepare and transform data, ensure consistency, stage data in preferred destinations, enable reuse, and make it easy to update the data.","source":"1-introduction"},{"Q":"What is the benefit of using dataflows in a retail company?","A":"Using dataflows allows data engineers to consolidate disparate data sources from different stores, prepare the data for analysis and reporting, ensure consistency, stage the data, enable reuse, and easily update the data.","source":"1-introduction"},{"Q":"What is the alternative to using dataflows for data extraction and transformation?","A":"The alternative to using dataflows is manually extracting and transforming data from every source, which is time-consuming and prone to errors.","source":"1-introduction"},{"Q":"What is the purpose of Dataflows (Gen2)?","A":"To develop a data model that can standardize the data and provide access to the business.","source":"2-dataflows-gen-2"},{"Q":"What are Dataflows (Gen2)?","A":"They are a type of cloud-based ETL tool for building and executing scalable data transformation processes.","source":"2-dataflows-gen-2"},{"Q":"How can you use Dataflows (Gen2)?","A":"You can connect to various data sources, transform the data, and load it into a destination.","source":"2-dataflows-gen-2"},{"Q":"What are the benefits of using Dataflows (Gen2)?","A":"Extend data with consistent data, allow self-service users access to a subset of data, optimize performance, simplify data source complexity, ensure data consistency and quality, and simplify data integration.","source":"2-dataflows-gen-2"},{"Q":"What are the limitations of using Dataflows (Gen2)?","A":"Not a replacement for a data warehouse, does not support row-level security, and requires a Fabric capacity workspace.","source":"2-dataflows-gen-2"},{"Q":"Where can you create a Dataflow (Gen2) in Microsoft Fabric?","A":"In the Data Factory workload or Power BI workspace, or directly in the lakehouse.","source":"3-explore-dataflows-gen-2"},{"Q":"What is used to visualize transformations in Dataflows (Gen2)?","A":"Power Query Online.","source":"3-explore-dataflows-gen-2"},{"Q":"What are some common data transformations in Dataflows (Gen2)?","A":"Filter and Sort rows, Pivot and Unpivot, Merge and Append queries, Split and Conditional split, Replace values and Remove duplicates, Add, Rename, Reorder, or Delete columns, Rank and Percentage calculator, Top N and Bottom N.","source":"3-explore-dataflows-gen-2"},{"Q":"What does the Queries pane in Power Query Online show?","A":"The different data sources or queries.","source":"3-explore-dataflows-gen-2"},{"Q":"What does the Diagram View in Power Query Online allow you to do?","A":"Visually see how the data sources are connected and the applied transformations.","source":"3-explore-dataflows-gen-2"},{"Q":"What are dataflows?","A":"Dataflows are useful for data transformations in Microsoft Fabric.","source":"4-dataflow-pipeline"},{"Q":"What are some common activities in data engineering?","A":"Some common activities include copying data, incorporating dataflow, adding a notebook, getting metadata, and executing a script or stored procedure.","source":"4-dataflow-pipeline"},{"Q":"How are pipelines created in Data Factory and Data Engineering workloads?","A":"Pipelines are easily created.","source":"4-dataflow-pipeline"},{"Q":"What are pipelines used for?","A":"Pipelines provide a visual way to complete activities in a specific order.","source":"4-dataflow-pipeline"},{"Q":"How can dataflows be incorporated into pipelines?","A":"Dataflows can be incorporated into a pipeline to orchestrate extra activities after the dataflow has completed.","source":"4-dataflow-pipeline"}],"get-started-data-warehouse":[{"Q":"What is the role of relational data warehouses in enterprise business intelligence?","A":"Relational data warehouses are at the center of most enterprise business intelligence (BI) solutions.","source":"1-introduction"},{"Q":"What is the standard design for a relational data warehouse?","A":"A common pattern based on a denormalized, multidimensional schema.","source":"1-introduction"},{"Q":"What is unique about Microsoft Fabric\'s data warehouse?","A":"It\'s built on the Lakehouse, which is stored in Delta format and can be queried using SQL.","source":"1-introduction"},{"Q":"Who can use Fabric\'s data warehouse?","A":"The whole data team, not just data engineers.","source":"1-introduction"},{"Q":"What are the benefits of using Fabric\'s data warehouse?","A":"It enables data engineers, analysts, and data scientists to work together to create and query a data warehouse that is optimized for their specific needs.","source":"1-introduction"},{"Q":"What are the steps involved in building a modern data warehouse?","A":"Data ingestion, data storage, data processing, data analysis and delivery.","source":"2-understand-data-warehouse"},{"Q":"What is Fabric\'s data warehouse experience?","A":"A relational data warehouse that supports T-SQL capabilities and can be used to store and query data.","source":"2-understand-data-warehouse"},{"Q":"What are fact tables in a data warehouse?","A":"Fact tables contain numerical data that you want to analyze.","source":"2-understand-data-warehouse"},{"Q":"What are dimension tables in a data warehouse?","A":"Dimension tables contain descriptive information about the data in the fact tables.","source":"2-understand-data-warehouse"},{"Q":"What is the purpose of surrogate and alternate keys in a data warehouse?","A":"Surrogate keys help maintain consistency and accuracy, while alternate keys help maintain traceability between the data warehouse and the source system.","source":"2-understand-data-warehouse"},{"Q":"What is Fabric\'s Lakehouse?","A":"Fabric\'s Lakehouse is a collection of files, folders, tables, and shortcuts that act like a database over a data lake.","source":"3-understand-data-warehouse-fabric"},{"Q":"What is the purpose of the data warehouse experience in Fabric?","A":"The data warehouse experience in Fabric allows you to build a relational layer on top of physical data in the Lakehouse and expose it to analysis and reporting tools.","source":"3-understand-data-warehouse-fabric"},{"Q":"How can you create a data warehouse in Fabric?","A":"You can create a data warehouse directly in Fabric from the create hub or within a workspace.","source":"3-understand-data-warehouse-fabric"},{"Q":"What are the ways to ingest data into a Fabric data warehouse?","A":"The ways to ingest data into a Fabric data warehouse include Pipelines, Dataflows, cross-database querying, and the COPY INTO command.","source":"3-understand-data-warehouse-fabric"},{"Q":"What are staging tables in a data warehouse?","A":"Staging tables are temporary tables used to perform data cleansing, transformations, and validation before loading the data into dimension and fact tables.","source":"3-understand-data-warehouse-fabric"},{"Q":"What are the two ways to query data from the data warehouse?","A":"Visual query editor and SQL query editor","source":"4-query-transform-data"},{"Q":"What is the SQL query editor?","A":"The SQL query editor provides a query experience with features like intellisense, code completion, syntax highlighting, etc.","source":"4-query-transform-data"},{"Q":"How can you create a new query in the SQL query editor?","A":"Use the New SQL query button in the menu","source":"4-query-transform-data"},{"Q":"What is the Visual query editor?","A":"The Visual query editor provides an experience similar to the Power Query online diagram view","source":"4-query-transform-data"},{"Q":"What can you do in the Visual query editor?","A":"You can drag a table to the canvas, add columns, filters, and other transformations to the query","source":"4-query-transform-data"},{"Q":"What is a data model?","A":"A data model defines the relationships between tables, rules for data aggregation, and measures used to derive insights.","source":"5-model-data"},{"Q":"How can you switch between Data, Query, and Model view in Fabric?","A":"You can switch using the menu in the bottom left corner of the screen.","source":"5-model-data"},{"Q":"What are relationships in a data model?","A":"Relationships allow you to connect tables in the data model.","source":"5-model-data"},{"Q":"How do you create measures in Fabric?","A":"You can create measures using the New measure button in the Model view.","source":"5-model-data"},{"Q":"What is a default dataset?","A":"A default dataset is automatically created in Fabric and inherits business logic from the parent lakehouse or warehouse.","source":"5-model-data"},{"Q":"What are some security features provided by Fabric for data warehouse?","A":"Role-based access control (RBAC), SSL encryption, Azure Storage Service Encryption, Azure Monitor, Azure Log Analytics, Multi-Factor Authentication (MFA), Microsoft Entra integration.","source":"6-security-monitor"},{"Q":"What is the purpose of workspace roles in securing data warehouse?","A":"Workspace roles are used to control access and manage the lifecycle of data and services in Fabric.","source":"6-security-monitor"},{"Q":"How can item permissions be used to grant access to individual warehouses?","A":"Item permissions can be granted via T-SQL or in the Fabric portal.","source":"6-security-monitor"},{"Q":"What are some permissions that can be granted to users for accessing the data warehouse?","A":"Read, ReadData, ReadAll.","source":"6-security-monitor"},{"Q":"What are some ways to monitor the data warehouse in Fabric?","A":"Use dynamic management views (DMVs) and query monitoring using sys.dm_exec_connections, sys.dm_exec_sessions, and sys.dm_exec_requests.","source":"6-security-monitor"}],"administer-fabric":[{"Q":"What tasks are essential for administering Fabric?","A":"A range of tasks","source":"1-introduction"},{"Q":"What do Fabric admins need to have a solid understanding of?","A":"Fabric architecture, security and governance features, analytics capabilities, and various deployment and licensing options available","source":"1-introduction"},{"Q":"Who do Fabric admins work closely with?","A":"Business users, data analysts, and other IT professionals","source":"1-introduction"},{"Q":"What is the role of a Fabric admin?","A":"To ensure that Fabric is deployed and used in a way that meets business objectives and complies with organizational policies and standards","source":"1-introduction"},{"Q":"What will you learn by the end of this module?","A":"An understanding of the Fabric administrator role and the tasks and tools involved in administering Fabric","source":"1-introduction"},{"Q":"What is Microsoft Fabric?","A":"Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, real-time analytics, and business intelligence.","source":"2-fabric-architecture"},{"Q":"What are the services offered by Microsoft Fabric?","A":"Microsoft Fabric offers a comprehensive suite of services, including data warehousing, data engineering, data integration, data science, real-time analytics, and business intelligence.","source":"2-fabric-architecture"},{"Q":"What is OneLake?","A":"OneLake is the native store used by all the Fabric experiences without needing any additional configuration. It is hierarchical in nature and provides a single-pane-of-glass file-system namespace that spans across users, regions, and clouds.","source":"2-fabric-architecture"},{"Q":"What is a Fabric tenant?","A":"A Fabric tenant is a dedicated space for organizations to create, store, and manage Fabric items. It maps to the root of OneLake and is at the top level of the hierarchy.","source":"2-fabric-architecture"},{"Q":"What is a workspace in Fabric?","A":"A workspace is a collection of items in Fabric that brings together different functionality in a single tenant. It acts as a container that leverages capacity for the work that is executed.","source":"2-fabric-architecture"},{"Q":"What are some admin tasks in Fabric administration?","A":"Security and access control, data governance, customization and configuration, monitoring and optimization","source":"3-admin-role-tools"},{"Q":"What are some tools used in Fabric administration?","A":"Fabric admin portal, PowerShell cmdlets, admin APIs and SDKs, admin monitoring workspace","source":"3-admin-role-tools"},{"Q":"What can you do in the Fabric admin portal?","A":"Manage settings for the entire tenant or by capacity, manage users and groups, access audit logs, monitor usage and performance","source":"3-admin-role-tools"},{"Q":"What can you do with PowerShell cmdlets in Fabric administration?","A":"Automate administrative tasks like managing groups, configuring data sources and gateways, monitoring usage and performance","source":"3-admin-role-tools"},{"Q":"What are admin APIs and SDKs used for in Fabric administration?","A":"Interacting programmatically with Fabric, automating administrative tasks, integrating Fabric with other systems","source":"3-admin-role-tools"},{"Q":"What is the role of an admin in Microsoft Fabric security?","A":"To manage security for the Fabric environment, including managing users and groups, and managing how users interact with Fabric in terms of sharing and distribution of content.","source":"4-manage-security"},{"Q":"What are licenses used for in the Fabric environment?","A":"Licenses control the level of access and functionality that users have within the Fabric environment.","source":"4-manage-security"},{"Q":"How can administrators ensure efficient allocation of licenses in Fabric?","A":"By monitoring and controlling costs, and allocating licenses only to users who need them.","source":"4-manage-security"},{"Q":"Where is license management for Fabric handled?","A":"In the Microsoft 365 admin center.","source":"4-manage-security"},{"Q":"What can admins manage in terms of content sharing and distribution in Fabric?","A":"Admins can manage how users share content with others, and how they distribute content to others.","source":"4-manage-security"},{"Q":"What is endorsement in Fabric?","A":"Endorsement is a way to designate specific Fabric items as trusted and approved for use across the organization.","source":"5-govern-fabric"},{"Q":"What are the governance features in Fabric?","A":"Fabric has built-in governance features to help manage and control data.","source":"5-govern-fabric"},{"Q":"What is the scanner API used for?","A":"The scanner API is used to scan Fabric items for sensitive data.","source":"5-govern-fabric"},{"Q":"What is data lineage?","A":"Data lineage is the ability to track the flow of data through Fabric.","source":"5-govern-fabric"},{"Q":"Who can promote content within a workspace in Fabric?","A":"Workspace members with the contributor or admin role can promote content within a workspace.","source":"5-govern-fabric"}],"describe-medallion-architecture":[{"Q":"What is the purpose of the medallion architecture?","A":"To bring structure and efficiency to the lakehouse environment.","source":"1-introduction"},{"Q":"What are the three layers of the medallion architecture?","A":"Bronze, silver, and gold.","source":"1-introduction"},{"Q":"Who is the target audience for this module?","A":"Both seasoned data engineers and analytics newcomers.","source":"1-introduction"},{"Q":"What are the prerequisites for starting this module?","A":"Familiarity with the Fabric data lakehouse and basic understanding of SQL and Power BI.","source":"1-introduction"},{"Q":"What will you learn in this module?","A":"You will learn how to explore and build a medallion architecture, query and report on data, and describe best practices for security and governance.","source":"1-introduction"},{"Q":"What is the medallion architecture?","A":"The medallion architecture is a recommended data design pattern used to organize data in a lakehouse logically.","source":"2-describe-medallion-architecture"},{"Q":"What are the three layers of the medallion architecture?","A":"The three layers are bronze (raw), silver (validated), and gold (enriched).","source":"2-describe-medallion-architecture"},{"Q":"What is the purpose of the bronze layer?","A":"The bronze layer is the landing zone for all data, where no changes are made to it.","source":"2-describe-medallion-architecture"},{"Q":"What activities are typically done in the silver layer?","A":"Activities in the silver layer include combining and merging data, enforcing data validation rules, and cleaning the data.","source":"2-describe-medallion-architecture"},{"Q":"What is the purpose of the gold layer?","A":"The gold layer refines the data further to align with specific business and analytics needs, making it ready for use by downstream teams.","source":"2-describe-medallion-architecture"},{"Q":"How do you set up the foundation for the medallion architecture?","A":"Create your Fabric lakehouse.","source":"3-implement-medallion-archecture-fabric"},{"Q":"What are the three layers in the medallion architecture?","A":"Bronze, Silver, and Gold.","source":"3-implement-medallion-archecture-fabric"},{"Q":"What is the purpose of the Bronze layer?","A":"To ingest raw data.","source":"3-implement-medallion-archecture-fabric"},{"Q":"How can you transform data in the Silver layer?","A":"Using dataflows or notebooks.","source":"3-implement-medallion-archecture-fabric"},{"Q":"How can you enable downstream consumption of the data?","A":"By connecting to the SQL endpoint.","source":"3-implement-medallion-archecture-fabric"},{"Q":"What tools and technologies can you use to query and report on data in Fabric?","A":"SQL endpoints and Direct Lake mode in Power BI datasets.","source":"4-query-report-data"},{"Q":"How can you explore and query data in the gold layer?","A":"Using SQL and T-SQL language, saving functions, generating views, and applying SQL security.","source":"4-query-report-data"},{"Q":"What mode does the SQL endpoint in Fabric operate in?","A":"Read-only mode.","source":"4-query-report-data"},{"Q":"What is Direct Lake mode in Power BI datasets?","A":"It is an approach that combines the performance of a semantic model with the freshness of lakehouse data.","source":"4-query-report-data"},{"Q":"How can you tailor medallion layers for different needs?","A":"By customizing the layers\' structure and organization to align with the requirements of specific user groups.","source":"4-query-report-data"},{"Q":"How can you secure your lakehouse?","A":"Set permissions at the workspace or item level.","source":"5-secure-govern"},{"Q":"What are the considerations for securing a lakehouse?","A":"Security and access considerations, gold layer access control, silver layer utilization, and bronze layer access control.","source":"5-secure-govern"},{"Q":"What should be discussed with the security team when sharing Fabric content?","A":"Ensure it aligns with your organization\'s security policies.","source":"5-secure-govern"},{"Q":"What considerations are involved in designing a CI/CD process for a lakehouse architecture?","A":"Data quality checks, version control, automated deployments, monitoring, and security measures.","source":"5-secure-govern"},{"Q":"Why is CI/CD crucial at the gold layer of a lakehouse?","A":"It ensures high-quality, validated, and reliable data for consumption.","source":"5-secure-govern"}],"ingest-data-with-spark-fabric-notebooks":[{"Q":"What is the first step when choosing Microsoft Fabric for analytics?","A":"Seamlessly ingesting data into the Fabric lakehouse.","source":"1-introduction"},{"Q":"What does ETL stand for?","A":"Extract, transform, load.","source":"1-introduction"},{"Q":"When using Fabric notebooks, what can you do with external data?","A":"Extract, load, and transform the data.","source":"1-introduction"},{"Q":"What are Fabric notebooks best suited for?","A":"Handling large external datasets and performing complex transformations.","source":"1-introduction"},{"Q":"What will you learn by the end of this module?","A":"Using Spark and Fabric notebooks to ingest external data and optimizing the ETL process.","source":"1-introduction"},{"Q":"What advantages do Fabric notebooks offer over other ingestion options?","A":"Automation and systematic approach","source":"2-connect-authenticate"},{"Q":"Where are Fabric notebooks stored?","A":"In the workspace they\'re created from","source":"2-connect-authenticate"},{"Q":"What languages can be used in Fabric notebooks?","A":"PySpark, Html, Spark (Scala), Spark SQL, and SparkR (R)","source":"2-connect-authenticate"},{"Q":"What is the default engine used in Fabric notebooks?","A":"PySpark","source":"2-connect-authenticate"},{"Q":"How can you connect to Azure blob storage with Spark in a Fabric notebook?","A":"Using the Azure Blob Storage access info and constructing the connection path","source":"2-connect-authenticate"},{"Q":"How can you save data into a lakehouse?","A":"You can save data either as a file or load it as a Delta table.","source":"3-write-optimize"},{"Q":"What formats does a lakehouse support?","A":"A lakehouse supports structured, semi-structured, and unstructured files. Parquet and Delta tables are commonly used.","source":"3-write-optimize"},{"Q":"What is the preferred format for saving data in a lakehouse?","A":"Parquet is the preferred format due to its optimized columnar storage structure and compression capabilities.","source":"3-write-optimize"},{"Q":"How can you load data into a Delta table?","A":"You can load data into a Delta table using the `saveAsTable` method with the format set to \'delta\'.","source":"3-write-optimize"},{"Q":"What are some optimization functions for Delta table writes?","A":"V-Order and Optimize write are optimization functions that can be used to improve Delta table write performance.","source":"3-write-optimize"},{"Q":"What is the bronze layer in a Medallion architecture called?","A":"Fabric lakehouse","source":"4-considerations"},{"Q":"What are some basic cleaning steps when loading data?","A":"Removing duplicates, handling errors, converting null values, and getting rid of empty entries","source":"4-considerations"},{"Q":"What is the role of Fabric\'s Data Wrangler?","A":"To let data scientists explore the data and generate transformation code","source":"4-considerations"},{"Q":"What are the requirements of Power BI data analysts?","A":"More transformation and modeling before they can use the data","source":"4-considerations"},{"Q":"What does the module \'Use Apache Spark in Microsoft Fabric\' teach?","A":"How to use Fabric notebooks to display, aggregate, and transform data with Spark","source":"4-considerations"}],"get-started-kusto-fabric":[{"Q":"What is Microsoft Fabric?","A":"Microsoft Fabric is an end-to-end platform for data solutions, including real-time data analytics.","source":"1-introduction"},{"Q":"What is Synapse Real-Time Analytics?","A":"Synapse Real-Time Analytics in Fabric uses a KQL Database to provide table storage and Kusto Query Language for data analysis.","source":"1-introduction"},{"Q":"What is KQL?","A":"KQL stands for Kusto Query Language which is a powerful tool for analyzing data.","source":"1-introduction"},{"Q":"What is the benefit of using KQL for data analysis?","A":"KQL provides an efficient way to find insights and patterns from textual or structured data, especially for data with a time series component.","source":"1-introduction"},{"Q":"What can you do with Real-Time Analytics in Microsoft Fabric?","A":"With Real-Time Analytics, you can scale up your analytics solution and democratize data for the needs of your entire data organization.","source":"1-introduction"},{"Q":"What is Real-Time Analytics in Microsoft Fabric?","A":"Real-Time Analytics is a fully managed service optimized for streaming time-series data.","source":"2-define-real-time-analytics"},{"Q":"What are the benefits of using Real-Time Analytics?","A":"The benefits of using Real-Time Analytics include ingesting data from any source in any format, running analytical queries directly on raw data, and working with versatile data structures.","source":"2-define-real-time-analytics"},{"Q":"What is Kusto Query Language (KQL)?","A":"Kusto Query Language (KQL) is a declarative query language used to analyze and extract insights from structured, semi-structured, and unstructured data.","source":"2-define-real-time-analytics"},{"Q":"What are the benefits of using KQL in Microsoft Fabric?","A":"The benefits of using KQL in Microsoft Fabric include efficiency in data exploration and analysis, reproducible analyses, and improving DevOps troubleshooting experience.","source":"2-define-real-time-analytics"},{"Q":"What can you do with KQL in Microsoft Fabric?","A":"With KQL in Microsoft Fabric, you can filter, present, aggregate your data, and quickly paste long, complex queries directly into the editor.","source":"2-define-real-time-analytics"},{"Q":"What is the purpose of a KQL database?","A":"A KQL database hosts a collection of tables, stored functions, materialized views, shortcuts, and datastreams.","source":"3-describe-kusto-databases-tables"},{"Q":"What can you do with the KQL Queryset?","A":"The KQL Queryset allows you to run queries, view and manipulate query results, save queries for future use, and export and share queries with others.","source":"3-describe-kusto-databases-tables"},{"Q":"What is the Eventstream feature in Synapse Real-Time Analytics?","A":"The Eventstream feature allows you to integrate streaming data from multiple source types and send it to multiple destinations, such as a Lakehouse, KQL Database, or a custom app.","source":"3-describe-kusto-databases-tables"},{"Q":"What is a table in a KQL Database?","A":"A table is a schema entity that contains a set of columns and rows of data.","source":"3-describe-kusto-databases-tables"},{"Q":"What is a materialized view in a KQL Database?","A":"A materialized view is a schema entity that stores precomputed results of a query for faster retrieval.","source":"3-describe-kusto-databases-tables"},{"Q":"What is KQL?","A":"KQL stands for Kusto Query Language and is used to write queries in Azure Data Explorer, Azure Monitor Log Analytics, Azure Sentinel, and Azure Fabric.","source":"4-write-queries-kusto-query-language"},{"Q":"How do you create a table in KQL?","A":"You can use the `.create table` command followed by the table name, column names, their data types, and optional properties.","source":"4-write-queries-kusto-query-language"},{"Q":"How do you ingest data into a table in KQL?","A":"You can use the `ingest into` command followed by the table name and the data source URL, with optional parameters.","source":"4-write-queries-kusto-query-language"},{"Q":"How do you retrieve all data from a table in KQL?","A":"You can simply write the table name as a query statement, like `sales`.","source":"4-write-queries-kusto-query-language"},{"Q":"How do you filter data in KQL?","A":"You can use the `where` clause followed by a Boolean expression to filter rows based on a specified condition.","source":"4-write-queries-kusto-query-language"}],"explore-event-streams-microsoft-fabric":[{"Q":"What is the first step in using Microsoft Fabric for real-time analytics?","A":"Seamlessly ingesting data into the Fabric lakehouse.","source":"1-introduction"},{"Q":"What is Eventstreams in Microsoft Fabric?","A":"A feature that allows you to handle real-time events without coding.","source":"1-introduction"},{"Q":"What are some tasks you can perform using Eventstreams?","A":"You can set up event sources, destinations, and processors, and perform event processing for collecting and aggregating data.","source":"1-introduction"},{"Q":"What is the eventstream main editor used for?","A":"Setting up Fabric eventstreams, establishing sources and destinations, viewing data in-flight, and capturing, transforming, and routing data.","source":"1-introduction"},{"Q":"What can you do with Fabric eventstreams?","A":"You can source, transform, process, and route event driven data to the necessary destinations for your workloads.","source":"1-introduction"},{"Q":"What is Eventstream?","A":"Eventstream is a no-code feature that lets you capture and send real-time events to different places.","source":"2-eventstream-components"},{"Q":"Why should I use Eventstream?","A":"You should use Eventstream if you want to use real-time data in a simple way.","source":"2-eventstream-components"},{"Q":"How does Eventstream work?","A":"Eventstream works by creating a pipeline of events from multiple sources to different destinations.","source":"2-eventstream-components"},{"Q":"What are the main components of Eventstream?","A":"The main components of Eventstream are Eventstream, Source, Destination, and Main Editor.","source":"2-eventstream-components"},{"Q":"What are the benefits of Eventstream?","A":"Some benefits of Eventstream are no-code experience, source connectors, multiple destinations, scalable infrastructure, and interoperability with Azure services.","source":"2-eventstream-components"},{"Q":"What are the available event sources in Eventstream?","A":"The available event sources in Eventstream are Azure Event Hubs, sample data, and custom app.","source":"3-setup-eventstreams"},{"Q":"How do you create an event source in Eventstream?","A":"To create an event source, you need to change your Fabric experience to Real-time Analytics and select Eventstream, then follow the steps mentioned in the tutorial.","source":"3-setup-eventstreams"},{"Q":"What is Azure Event Hubs?","A":"Azure Event Hubs is a source that allows you to get event data from an Azure event hub.","source":"3-setup-eventstreams"},{"Q":"How can you use sample data as an event source in Eventstream?","A":"To use sample data as an event source, you need to select Sample data as the source type, choose the sample data set you want to add, and then select Add.","source":"3-setup-eventstreams"},{"Q":"What are the steps to stream events from your own application using Custom app as an event source?","A":"To stream events from your own application using Custom app as an event source, you need to create an eventstream item, add a Custom app source to it, create an application that sends events to the eventstream, and add destinations to route and transform the event data.","source":"3-setup-eventstreams"},{"Q":"How do you add an eventstream destination in Microsoft Fabric?","A":"Select New destination on the ribbon or ![Screenshot of new button in main editor.](../media/new-button-main-editor.png) in the main editor canvas and then select the type of destination you want to add.","source":"4-route-event-data-to-destinations"},{"Q":"What are the types of event destinations available in eventstream?","A":"The types of event destinations available in eventstream are KQL Database, Lakehouse, Custom App, and Reflex.","source":"4-route-event-data-to-destinations"},{"Q":"How can you store events in a delta table using Lakehouse as an event destination?","A":"To store events in a delta table using Lakehouse, you need to create a lakehouse and an eventstream in your workspace, then add a lakehouse destination to your eventstream and configure it.","source":"4-route-event-data-to-destinations"},{"Q":"What are some event processor operations available in eventstream?","A":"Some event processor operations available in eventstream are Aggregate, Expand, Filter, Group by, Manage fields, and Union.","source":"4-route-event-data-to-destinations"},{"Q":"What are windowing functions in eventstream?","A":"Windowing functions in eventstream are a way to perform operations on data contained in temporal windows, such as aggregating, filtering, or transforming streaming events based on time.","source":"4-route-event-data-to-destinations"}]}'),v=JSON.parse('[{"title":"Introduction to end-to-end analytics using Microsoft Fabric","summary":"Discover how Microsoft Fabric can meet your enterprise\'s analytics needs in one platform. Learn about Microsoft Fabric, how it works, and identify how you can use it for your analytics needs.","module":"introduction-end-analytics-use-microsoft-fabric"},{"title":"Get started with lakehouses in Microsoft Fabric","summary":"Lakehouses merge data lake storage flexibility with data warehouse analytics. Microsoft Fabric offers a lakehouse solution for comprehensive analytics on a single SaaS platform.","module":"get-started-lakehouses"},{"title":"Use Apache Spark in Microsoft Fabric","summary":"Apache Spark is a core technology for large-scale data analytics. Microsoft Fabric provides support for Spark clusters, enabling you to analyze and process data in a Lakehouse at scale.","module":"use-apache-spark-work-files-lakehouse"},{"title":"Work with Delta Lake tables in Microsoft Fabric","summary":"Tables in a Microsoft Fabric lakehouse are based on the Delta Lake storage format commonly used in Apache Spark. By using the enhanced capabilities of delta tables, you can create advanced analytics solutions.","module":"work-delta-lake-tables-fabric"},{"title":"Use Data Factory pipelines in Microsoft Fabric","summary":"Microsoft Fabric includes Data Factory capabilities, including the ability to create pipelines that orchestrate data ingestion and transformation tasks.","module":"use-data-factory-pipelines-fabric"},{"title":"Ingest Data with Dataflows Gen2 in Microsoft Fabric","summary":"Data ingestion is crucial in analytics. Microsoft Fabric\'s Data Factory offers Dataflows (Gen2) for visually creating multi-step data ingestion and transformation using Power Query Online.","module":"use-dataflow-gen-2-fabric"},{"title":"Get started with data warehouses in Microsoft Fabric","summary":"Data warehouses are analytical stores built on a relational schema to support SQL queries. Microsoft Fabric enables you to create a relational data warehouse in your workspace and integrate it easily with other elements of your end-to-end analytics solution.","module":"get-started-data-warehouse"},{"title":"Administer Microsoft Fabric","summary":"Microsoft Fabric is a SaaS solution for end-to-end data analytics. As an administrator, you can configure features and manage access to suit your organization\'s needs.","module":"administer-fabric"},{"title":"Organize a Fabric lakehouse using medallion architecture design","summary":"Explore the potential of the medallion architecture design in Microsoft Fabric. Organize and transform your data across Bronze, Silver, and Gold layers of a lakehouse for optimized analytics.","module":"describe-medallion-architecture"},{"title":"Ingest data with Spark and Microsoft Fabric notebooks","summary":"Discover how to use Apache Spark and Python for data ingestion into a Microsoft Fabric lakehouse. Fabric notebooks provide a scalable and systematic solution.","module":"ingest-data-with-spark-fabric-notebooks"},{"title":"Get started with Real-Time Analytics in Microsoft Fabric","summary":"Analysis of real-time data streams is a critical capability for any modern data analytics solution. You can use the Real-Time Analytics capabilities of Microsoft Fabric to ingest, query, and process streams of data.","module":"get-started-kusto-fabric"},{"title":"Use real time eventstreams in Microsoft Fabric","summary":"This module is an introduction to the Microsoft Fabric Eventstream within Real-Time Analytics (RTA)","module":"explore-event-streams-microsoft-fabric"}]');function Home(){let[e,a]=(0,r.useState)("get-started-lakehouses"),[t,i]=(0,r.useState)("Get started with lakehouses in Microsoft Fabric");return(0,o.jsxs)("div",{className:"p-4",children:[(0,o.jsx)("h1",{className:"text-6xl font-bold mb-4 text-center dark:text-white",children:"Fabric Flashcards"}),(0,o.jsx)("h2",{className:"text-2xl font-bold mb-4 text-center dark:text-white",children:"Select a topic to get started"}),(0,o.jsx)(Topic,{topics:v,onTopicChange:a,onModuleTitleChange:i}),(0,o.jsx)(FlashCard,{QandAs:k,topic:e,moduleTitle:t})]})}}},function(e){e.O(0,[948,971,472,744],function(){return e(e.s=4576)}),_N_E=e.O()}]);